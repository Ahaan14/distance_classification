### 1. What are the common distance metrics used in distance-based classification algorithms?  
Common distance metrics include **Euclidean distance** (straight-line distance), **Manhattan distance** (sum of absolute differences), **Minkowski distance** (a generalization of Euclidean and Manhattan), and **Cosine similarity** (measures the angle between two vectors, used for text data).

---

### 2. What are some real-world applications of distance-based classification algorithms?  
Applications include **spam email detection**, **recommendation systems**, **image and handwriting recognition**, and **medical diagnosis** (e.g., predicting diseases based on patient features).

---

### 3. Explain various distance metrics.  
- **Euclidean distance**: Measures straight-line distance in multidimensional space.  
- **Manhattan distance**: Calculates the sum of absolute differences between coordinates.  
- **Minkowski distance**: A generalized version of Euclidean and Manhattan distances, controlled by a parameter \(p\).  
- **Cosine similarity**: Measures the cosine of the angle between two vectors, used for high-dimensional data like text.

---

### 4. What is the role of cross-validation in model performance?  
Cross-validation splits the dataset into training and testing subsets multiple times to evaluate the modelâ€™s performance. It helps prevent **overfitting**, ensures **generalizability**, and provides a robust estimate of model accuracy.

---

### 5. Explain variance and bias in terms of KNN?  
In KNN, **high bias** occurs with large \(k\) values, leading to underfitting, as the model oversimplifies. **High variance** occurs with small \(k\) values, leading to overfitting, as the model becomes too sensitive to noise. Finding the right \(k\) balances bias and variance.
